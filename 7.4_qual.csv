Category,Type,Quote,Criteria ,Direction,QA  Status
Actor,AI Developer (General-purpose AI),General-purpose AI developers are primarily responsible as their design choices directly affect model transparency across applications.,responsibility,higher,complete
Actor,AI Developer (General-purpose AI),AI developer was rated as primarily responsible because they are often the only ones in the pipeline that have the actual capability to make AI decisions transparent and interpretable,responsibility,higher,complete
Actor,AI Developer (General-purpose AI),[1] AI Developers - Primarily Responsible: I rated AI developers as primarily responsible because they are the actors who implement the prompt revision processes and design the system prompts that govern model behavior. They have direct control over these mechanisms and therefore bear the primary responsibility for being transparent about what system prompts they use and how their prompt revision processes function.,responsibility,higher,complete
Actor,AI Developer (General-purpose AI),"AI Developers are responsible for the quality of their products, and transparency/interpretability can greatly assist in evaluating the quality.",responsibility,higher,complete
Actor,AI Developer (General-purpose AI),"Again, AI developers are primarily responsible since they would be the ones researching foundational interpretability methods. Deployers and governance have some influence over setting standards, but developers are the ones actually researching these methods.",responsibility,higher,complete
Actor,AI Developer (General-purpose AI),General-purpose AI Developers were marked as primarily responsible because they architect the foundational logic of systems used across sectors.,responsibility,higher,complete
Actor,AI Developer (General-purpose AI),Transparency and interpretability are ultimately technical issues at their base and so developers are most responsible along with deployers who have the opportunity to enhance interpretability and transparency in deploying the model.,responsibility,higher,complete
Actor,AI Developer (General-purpose AI),The primarily bottlenecks to transparency and interpretability sit at the model development stage. It is primarily up to developers and policymakers to ensure that the right technology gets developed.,responsibility,higher,complete
Actor,AI Developer (General-purpose AI),"Developers have the capability and causal influence to be transparent, and the obligation to. However, they usually will not due to fiduciary duties to stakeholders that incentivize these organizations to limit transparency to limit liability.",responsibility,lower,complete
Actor,AI Developer (General-purpose AI),"Technical transparency (developers), regulatory requirements (governance), and implementation practices (deployers) must all align.",general comment,general comment,complete
Actor,AI Developer (General-purpose AI),"Lower rates are given to either actors who do not have access to the technology to be able to practically address the risk or are enablers. Higher rates are given to actors who have responsibility in the design, development, and deployment of the AI systems and hence have the means to mitigate the lack of transparency and interpretability risk.",responsibility,higher,complete
Actor,AI Developer (Specialized AI),Specialized AI developers and deployers are highly responsible for ensuring interpretability in domain-specific contexts and for users' decision-making.,responsibility,higher,complete
Actor,AI Deployer,Specialized AI developers and deployers are highly responsible for ensuring interpretability in domain-specific contexts and for users' decision-making.,responsibility,higher,complete
Actor,AI Deployer,"[2] AI Deployers - Highly Responsible: I rated AI deployers as highly responsible because they have the power to make informed choices about which AI systems to implement. They can choose to use only open-sourced models with higher transparency and interpretability, or they can choose to collaborate exclusively with developers and companies who disclose their system prompts.",responsibility,higher,complete
Actor,AI Deployer,AI Deployers and Governance Actors are highly responsible because they choose how to implement and oversee these systems.,responsibility,higher,complete
Actor,AI Deployer,Transparency and interpretability are ultimately technical issues at their base and so developers are most responsible along with deployers who have the opportunity to enhance interpretability and transparency in deploying the model.,responsibility,higher,complete
Actor,AI Deployer,Deployers can help by stating the provenance of their data/model systems and the contexts they sourced it from and how they have tweaked it and expect it to be useful,responsibility,higher,complete
Actor,AI Deployer,"Without transparency, organizations developing and deploying AI can not be held accountable by AI Governance organizations... AI deployers are also at risk, as they may not know what they are actually deploying and making available to customers, and without clear liability guidance, they may be liable for any harms caused.",vulnerability,higher,complete
Actor,AI Governance Actor,"Governance actors also hold high responsibility, as they must enforce interpretability standards and accountability mechanisms.",responsibility,higher,complete
Actor,AI Governance Actor,Governance actors must ensure strict controls and testing around that.,responsibility,higher,complete
Actor,AI Governance Actor,"[3] AI Governance Actors - Primarily Responsible: I rated governance actors as primarily responsible because they possess the greatest power to address this issue through policy, regulation and law. They can push forward laws and policies that require companies to be transparent about their system prompts and prompt revision processes. Additionally, they can mandate that companies provide access to base model behavior without prompt revision at least in the model evaluation process, ensuring that evaluators and researchers can assess true model capabilities. Governance actors ultimately hold the most significant power in ensuring transparency and interpretability across the AI ecosystem.",responsibility,higher,complete
Actor,AI Governance Actor,"[2] AI Governance Actors - Extremely Vulnerable: AI governance actors face extreme vulnerability because many AI researchers and evaluators are unaware that prompts are being revised and do not know what system prompts govern the revision process or what system prompts AI companies use generally. This transparency gap raises questions about model evaluation: Are we assessing model behavior after prompt revision or before? Without reliable methods to disable revision processes and evaluate base model behavior, governance and evaluation become compromised. Evaluators may assess a model as reliable, but the automatic revision process could introduce unknown bias. They cannot predict how the model will behave if users circumvent the revision system, creating significant governance vulnerabilities.",vulnerability,higher,complete
Actor,AI Governance Actor,AI Deployers and Governance Actors are highly responsible because they choose how to implement and oversee these systems.,responsibility,higher,complete
Actor,AI Governance Actor,Governance actors are highly responsible for ensuring deployers and developers follow best practices to ensure these norms.,responsibility,higher,complete
Actor,AI Governance Actor,Therefore governments and policy makers are responsible for developing rules to ensure that these organizations are transparent.,responsibility,higher,complete
Actor,AI Governance Actor,"I think there's a category error by including AI Governance Actors in this assessment. They're different in kind of users developers and deployers. They're more like a sector, if anything.",general comment,general comment,complete
Actor,AI Infrastructure Provider,Infrastructure providers have moderate responsibility by enabling or constraining access to tools that support transparency.,responsibility,higher,complete
Actor,AI User,"Users and affected stakeholders have minimal or no responsibility, as they lack the structural capacity to address systemic transparency gaps",responsibility,lower,complete
Actor,AI User,"The only way transparency will be available is when users (consumers) demand it, as was the case with (SBoM's) or Software Bill of Materials.",responsibility,higher,complete
Actor,AI User,Users often lack insights that could be gained from manufacturers making their algorithms clear and predictable.,responsibility,lower,complete
Actor,AI User,"AI users face extreme vulnerability due to lack of transparency in system prompts, which control what AI systems can and cannot say or do. Most companies, except Anthropic, do not disclose their system prompts, creating opportunities for encoded bias, discrimination, and unintended consequences. The automatic prompt revision process is a perfect example of the consequences of lack of transparency. Research has shown that users cannot control OpenAI's prompt revision process in their text-to-image models. When users submit prompts, they are not even aware that their prompts are being revised, OpenAI does not voluntarily provide the revised prompt as output on their web interface, and users cannot disable this process even when aware of it. This lack of transparency can distort reality on controversial topics. For example, when users request images of dairy or pig farms, the system consistently shows pastoral scenes rather than realistic depictions of intensive farming, reinforcing existing misconceptions. Only through specialized prompt engineering techniques can researchers bypass this process to reveal that the base model is actually aware of intensive farming realities. The prompt revision process erased that reality, and showed all cows on pasture and pigs in mud. There is no transparency in the system prompt used to govern the prompt revision process.",vulnerability,higher,complete
Actor,AI User,AI Users have some influence but often lack visibility into the system's designâ€š hence only moderately responsible.,responsibility,lower,complete
Actor,AI User,Users and stakeholders are not responsible for making tools interpretable or transparent but are responsible for understanding the pitfalls and limits in these areas and also responsible for being transparent about their own uses of AI to any downstream stakeholders.,responsibility,higher,complete
Actor,AI User,"taking its output as truth is the bug of the user, not the system and education on AI's capabilites should rest with regulators.",responsibility,higher,complete
Actor,Affected Stakeholder,"Users and affected stakeholders have minimal or no responsibility, as they lack the structural capacity to address systemic transparency gaps",responsibility,lower,complete
Actor,Affected Stakeholder,"Affected Stakeholders should not be held responsible, they are recipients of system outcomes, not decision-makers.",responsibility,lower,complete
Actor,Affected Stakeholder,Users and stakeholders are not responsible for making tools interpretable or transparent but are responsible for understanding the pitfalls and limits in these areas and also responsible for being transparent about their own uses of AI to any downstream stakeholders.,responsibility,lower,complete
Actor,Industry Bodies,"I believe that industry bodies should be highly responsible for this risk, as self-regulation is the most common approach in many jurisdictions, and industry bodies are empowered to establish industry standards that minimise common harms to each sector.",general comment,general comment,complete
Sector,"Information, Healthcare, and National Security","Lack of transparency and interpretability is most critical in information, healthcare, and national security, where opaque AI decisions may endanger lives, undermine strategic stability, or erode democratic accountability.",vulnerability,higher,complete
Sector,Finance and Professional Services,"Finance and professional services are also highly vulnerable due to the reliance on explainability for compliance, trust, and risk management.",vulnerability,higher,complete
Sector,Education and Research,"Education and research sectors face significant challenges as reproducibility, trust in knowledge, and accountability for teaching and research outputs depend on interpretability.",vulnerability,higher,complete
Sector,Real Estate or Accommodation,"Sectors like real estate or accommodation are less exposed, as decision-making impact is less systemic. ",vulnerability,higher,revisit - I think this quote needs more context 
Sector,Educational Services,"For educational services I labeled the vulnerability as moderate. This is because while I believe the sensitivity would be high due to the potential impact on vulnerable people's education, the exposure currently is relatively low since few educational institutions are using AI in critical domains",vulnerability,lower,complete
Sector,"Healthcare, Agriculture, National Security, and Education","Industries such as healthcare, agriculture, national security, and education face extreme vulnerability because they cannot reliably use AI systems in high-stakes situations without understanding the underlying rules and system prompts governing model behavior. In healthcare, practitioners cannot safely use AI for prescribing or diagnosing patients without understanding how the model makes the diagnosis. Similarly, educational services cannot responsibly deploy AI with students and teenagers without understanding what system prompts govern the model's responses and limitations.",vulnerability,higher,complete
Sector,All Sectors,"When the 'pioneer of neural networks' and 'godfather of AI' Dr. Geoffrey Hinton is astonished at the speed, capability and scale of AI, and admits that even the most advanced AI developers cannot fully explain AI behaviors and/or capabilities, it is fair to assess each of the sectors listed above are extremely vulnerable to a lack of transparency.",general vulnerability comment,higher,complete
Sector,All Sectors,"I set a high to extreme vulnerability for almost all sectors, because the lack of accountability and auditability can create serious organisational issues in any business or administration.",general vulnerability comment,higher,complete
Sector,All Sectors,none of the sectors CAN fully understand the black box system and thus are extremely vulnerable,general vulnerability comment,higher,complete
Sector,"Public Admin, Nat Sec, Information, Science, Finance, Education","Public admin, Nat Sec, Information, Science, finance etc are highly vulnerable because they involve significant requirements or expectations to be able to show provenance of ideas and decision making processes in order to justify high-impact decisions. Areas like education are highly vulnerable partly due to a need to justify decisions and be transparent about AI use, but more so because lack of transparency might lead to harms that affect individuals' education and long term outcomes which are even more high risk and long-term than a mistake in another sector.",vulnerability,higher,complete
Sector,All Sectors,"Ultimately any deployment of AI without transparency will be a significant issue for any sector, but the less vulnerable ones on this list are less likely to be able to deploy AI in ways that will significantly impact major elements of their work.",general vulnerability comment,higher,complete
Sector,"Information, Education, Culture, Scientific Research, Arts","Deep learning models are often biased because of the data they are trained on is also biased. The lack of interpretability makes it difficult to identify biased responses/decisions. This is especially risky for information, education, and culture (but also for scientific research). LLMs in particular tend to lie pretty consistently about their responses being biased and lack of interpretabilty jeopardizes trust. Imagine AI reading CVs, loan applications, and making biased decisions but lying about it when asked. via prompt. Arts is also vulnerable because without interpretability, it's difficult to perform 'data attribution', i.e., understanding what human art the model used to produce generated art and attributing the original authors.",vulnerability,higher,complete
Sector,"Information, Science, National Security","Generally, my ratings for interpretability are based off of level of complexity. For smaller less capable models doing less complex tasks, lack of interpretability is less dangerous. But for information, science and natsec decision can be very consequential, high-stakes, opaque and possibly integrate knowledge humans themselves do not/cannot easily verify.",vulnerability,higher,complete
Sector,All Sectors,Lack of transparency affects all AI deployments. Vulnerability primarily has to do with the stakes of the application.,general vulnerability comment,higher,complete
Sector,All Sectors,"Many sectors are moving toward requiring algorithmic transparency, making this a growing vulnerability across domains requiring high-stakes decision-making. As always, any sector highly dependent on LLMs or any AI systems is more affected, especially the end users.",general vulnerability comment,higher,complete
Sector,Healthcare and Critical Infrastructure,"Without transparency, organizations developing and deploying AI can not be held accountable by AI Governance organizations... They also may deploy systems that cause severe harm in critical industries, such as healthcare and critical infrastructure.",vulnerability,higher,complete
General,Commentary,I kept in mind recent cases such as SyRI and assessed the capability to defend oneself against black boxes,general comment,general comment,complete
General,Commentary,I kept in mind the power dynamics involved in the decisions of relying on black boxes instead of white boxes,general comment,general comment,complete
General,Commentary,Vulnerability is directly linked to repeatability and complexity of models.,general comment,general comment,complete
General,Commentary,Directly proportional to ownership of the actor.,general comment,general comment,complete
General,Commentary,"Rated vulnerability as the likelihood that low explainability/traceability will block auditing, contestability, or rapid error correction. Drivers: Proprietary/black-box models, missing data lineage, unlogged prompts/parameters, complex ensembles, non-interpretable features.",general comment,general comment,complete
General,Commentary,"Interpretability and transparency would build trust in AI systems, because they minimize many of the risks of dealing with inscrutable black boxen. Using a knowledge base or graph to ground model responses is a common approach to minimize risks from hallucinatory responses, so we have approaches already to mitigate some exposure. Using models as oracles for ground truth is not 100% effective; plan accordingly.",general comment,general comment,complete
General,Commentary,Based primarily on sensitivity to the quality of the input information and end-products.,general comment,general comment,complete
General,Commentary,"The primary harm I see is indirect and routes through misaligned goals, secret loyalties, and difficulties with using powerful AIs to make even more powerful AIs aligned. There are other more minor harms I am less an expert in.",general comment,general comment,complete
General,Commentary,"Focusing on interpretability wrt to misalignment and secret loyalties, the low tractability of interventions reduces responsibility.",general comment,general comment,complete
General,Commentary,"Vulnerability depends a lot on how willing a party or sector is to deploy an intransparent or unintepretable system to do something risky and then trust it anyway, which in turn depends on how easy they might be convinced that they should be willing to do this.",general comment,general comment,complete
General,Commentary,Handling this risk is mainly a matter or correctly informing actors that they should not be using intransparent or unexplainable systems for certain high-risk things.,general comment,general comment,complete
General,Commentary,"This is part of the same deeply entangled set of issues as 7.1 and 7.2, which is why my answers become identical.",general comment,general comment,complete
General,Commentary,"This collides with my assessments in other areas, so I would like to add that we have known for many years that neural networks are a black box and text generating transformers are no different. personally, I dont see this as an intrisic fault; many people speak with authority on a subject and their reasoning might be emotional baggage from their childhood or ideological indoctrination. Just because we dont understand how it reasons, doesnt make it bad.",general comment,general comment,complete
General,Commentary,"Current AI often requires choosing between accuracy and interpretability, making this a complex technical and policy challenge. Hence developeres and high level decision makers are mostly responsible.",general comment,general comment,complete
General,Commentary,"This is related to the AI misalignment / loss of control and concentration of power problems. Both problems are mostly (but not entirely!) caused by the fact that we can't actually tell the difference between a powerful future AGI-level system that is trying to do what it's supposed to be trying to do, vs. one that might do something different in the future, vs. one that already has a hidden agenda.",general comment,general comment,complete
General,Commentary,I think it concerns all actors in the long run as models on average become larger and more complex/capable.,general comment,general comment,complete
Summary,AI Developer (General-purpose AI),"Comments overwhelmingly emphasized GPAI developers as primarily responsible since they architect foundational logic, control prompt revision processes, and are the only ones with actual capability to make AI transparent, though some noted fiduciary duties incentivize limiting transparency to avoid liability.",Responsibility,Higher,
Summary,AI Developer (General-purpose AI),One perspective noted developers usually limit transparency due to fiduciary duties to stakeholders that incentivize avoiding liability.,Responsibility,Lower,
Summary,AI Developer (Specialized AI),Comments noted specialized developers share high responsibility with deployers for ensuring interpretability in domain-specific contexts critical for users' decision-making.,Responsibility,Higher,
Summary,AI Deployer,"Comments emphasized deployers' high responsibility through power to choose transparent systems, collaborate with developers who disclose system prompts, and enhance transparency when deploying models while stating data provenance.",Responsibility,Higher,
Summary,AI Deployer,Comments noted deployers face high vulnerability as they may not understand what they're deploying to customers and face liability risks without clear guidance on AI system behaviors.,Vulnerability,Higher,
Summary,AI Governance Actor,"Comments consistently rated governance actors as primarily to highly responsible, possessing greatest power through policy and regulation to mandate transparency requirements, enforce interpretability standards, and ensure organizations follow best practices.",Responsibility,Higher,
Summary,AI Governance Actor,"Comments described governance actors as extremely vulnerable because evaluators often don't know about prompt revision processes or system prompts, compromising their ability to assess true model behavior and predict circumvention risks.",Vulnerability,Higher,
Summary,AI Infrastructure Provider,Comments characterized infrastructure providers as moderately responsible by enabling or constraining access to tools that support transparency.,Responsibility,Higher,
Summary,AI User,"Most comments characterized users as minimally responsible, lacking structural capacity to address systemic transparency gaps and visibility into system design, though some noted responsibility for understanding limitations and being transparent about their own AI use.",Responsibility,Lower,
Summary,AI User,Some comments emphasized users bear responsibility when demanding transparency as consumers and for understanding AI limitations rather than taking outputs as truth.,Responsibility,Higher,
Summary,AI User,"Comments described users as extremely vulnerable due to hidden prompt revision processes, undisclosed system prompts creating opportunities for encoded bias, and inability to control or disable these processes even when aware of them.",Vulnerability,Higher,
Summary,Affected Stakeholder,"Comments consistently characterized affected stakeholders as not responsible since they are recipients of system outcomes rather than decision-makers, lacking structural capacity to address transparency gaps.",Responsibility,Lower,
Summary,"Agriculture, Mining, Construction and Manufacturing",No specific vulnerability comments provided for these sectors regarding transparency and interpretability.,Vulnerability,N/A,
Summary,"Trade, Transportation, and Utilities",No specific vulnerability comments provided for these sectors regarding transparency and interpretability.,Vulnerability,N/A,
Summary,Information,"Comments consistently rated information as highly to extremely vulnerable, where opaque AI decisions may erode democratic accountability, with biased responses difficult to identify and high-stakes decisions requiring provenance of ideas.",Vulnerability,Higher,
Summary,Finance and Insurance,"Comments emphasized finance as highly vulnerable due to reliance on explainability for compliance, trust, and risk management, with requirements to show provenance for high-impact decisions.",Vulnerability,Higher,
Summary,Real Estate and Rental and Leasing,Comments suggested real estate is less exposed as decision-making impact is less systemic compared to other sectors.,Vulnerability,Lower,
Summary,Professional and Technical Services,"Comments noted professional services as highly vulnerable due to reliance on explainability for compliance, trust, and risk management.",Vulnerability,Higher,
Summary,Scientific Research and Development,"Comments emphasized scientific research faces extreme vulnerability as reproducibility and trust in knowledge depend on interpretability, with difficulty in data attribution and verifying complex knowledge integration.",Vulnerability,Higher,
Summary,"Management, Administrative, and Support Services",No specific vulnerability comments provided for this sector regarding transparency and interpretability.,Vulnerability,N/A,
Summary,Educational Services,"Comments varied, with some rating education as moderately vulnerable due to low current AI adoption, while others emphasized extreme vulnerability due to impact on vulnerable populations' long-term outcomes and inability to responsibly deploy AI without understanding system prompts.",Vulnerability,Higher,
Summary,Educational Services,One comment rated education as moderately vulnerable due to currently low exposure despite high sensitivity for vulnerable populations.,Vulnerability,Lower,
Summary,Health Care and Social Assistance,"Comments unanimously rated healthcare as extremely vulnerable, emphasizing practitioners cannot safely use AI for prescribing or diagnosing without understanding how models make decisions, with potential for severe harm in critical healthcare applications.",Vulnerability,Higher,
Summary,"Arts, Entertainment, and Recreation",Comments noted arts and entertainment as vulnerable due to difficulty performing data attribution and understanding what human art models used to produce generated content.,Vulnerability,Higher,
Summary,"Accommodation, Food, and Other Services",No specific vulnerability comments provided for this sector regarding transparency and interpretability.,Vulnerability,N/A,
Summary,Public Administration excluding National Security,Comments emphasized public administration's high vulnerability due to requirements to show provenance of decisions and justify high-impact choices affecting public services.,Vulnerability,Higher,
Summary,National Security,"Comments consistently rated national security as extremely vulnerable where opaque AI decisions may endanger lives, undermine strategic stability, and involve consequential high-stakes decisions humans cannot easily verify.",Vulnerability,Higher,