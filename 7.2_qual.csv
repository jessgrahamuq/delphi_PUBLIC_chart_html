Category,Type,Quote,Criteria ,Direction,QA Status
Actor,AI Developer (General-purpose AI),"Responsibility for addressing dangerous AI capabilities sits primarily with AI developers (both general and specialized) and AI governance actors, as they design, release, and regulate models that could be exploited for large-scale harm.",responsibility,higher,complete
Actor,AI Developer (General-purpose AI),"I rated model makers primarily responsible because dangerous capabilities first and foremost occur when models are trained a certain way. Such capabilities are only present after extensive investment into training and often done without supervision, hence it falls first on model developers.",responsibility,higher,complete
Actor,AI Developer (General-purpose AI),General-purpose developers and deployers must shoulder primary responsibility due to their proximity to core capabilities and deployment vectors.,responsibility,higher,complete
Actor,AI Developer (General-purpose AI),Responsibility rests with the developers and the governance organizations responsible for compelling them to develop safe tools.,responsibility,higher,complete
Actor,AI Developer (General-purpose AI),"This is a weird one, vulnerability is indirect. One could also see it as harming AI developers as they would need to take more precautions.",vulnerability,higher,complete
Actor,AI Developer (General-purpose AI),A bit weird since it is not like it is easy to avoid dangerous capabilities for GPAIs. But a ban on AI progress and/or extreme unlearning could help.,general comment,general comment,complete
Actor,AI Governance Actor,Almost all dangerous capabilities stem from dual-use skills. Model developers do not have the right incentives for mitigating dangerous capabilities. Governance actors must craft policy here.,responsibility,higher,complete
Actor,AI Developer (General-purpose AI),"Once dangerous capabilities exist, containing them becomes exponentially harder. Primary responsibility lies with those who can prevent development or deployment. Both technical control (developers) and regulatory oversight (governance) are essential.",responsibility,higher,complete
Actor,AI Developer (General-purpose AI),"Only developers have reasonable capability to intervene on this, while only governments have a strong obligation to do so on the behalf of their citizens.",responsibility,higher,complete
Actor,AI Developer (Specialized AI),"Responsibility for addressing dangerous AI capabilities sits primarily with AI developers (both general and specialized) and AI governance actors, as they design, release, and regulate models that could be exploited for large-scale harm.",responsibility,higher,complete
Actor,AI Deployer,General-purpose developers and deployers must shoulder primary responsibility due to their proximity to core capabilities and deployment vectors.,responsibility,higher,complete
Actor,AI Deployer,Deployers are responsible for securing AI systems.,responsibility,higher,complete
Actor,AI Governance Actor,"Model deployers are under strong pressures to maximize realized capabilities in dangerous areas (e.g. cybersecurity), and arguably have little obligation to not maximize capabilities. Therefore, AI governance actors do.",responsibility,higher,complete
Actor,AI Deployer,"Deployers and infrastructure providers share high responsibility by controlling access, monitoring misuse, and enforcing policies.",responsibility,higher,complete
Actor,Commentary,"The deployer ecosystem is evolving rapidly into a multi-agent ecosystem with infrastructure, models, data, tools and agents all able to be provisioned a la carte. This increases the challenges of identifying accountable parties - whoever owns the workload is responsible for its activity, but determining accountability among shared responsibilities is challenging.",general responsibility comment,general responsibility comment,complete
Actor,AI Governance Actor,"Responsibility for addressing dangerous AI capabilities sits primarily with AI developers (both general and specialized) and AI governance actors, as they design, release, and regulate models that could be exploited for large-scale harm.",responsibility,higher,complete
Actor,AI Governance Actor,"The main risk driver is are bad and careless actors, markets will not solve this, governance solutions are needed.",responsibility,higher,complete
Actor,AI Governance Actor,Responsibility rests with the developers and the governance organizations responsible for compelling them to develop safe tools.,responsibility,higher,complete
Actor,AI Governance Actor,Governance bodies need to enforce provenance of digital activity in order to hold deployers accountable for risks here.,responsibility,higher,complete
Actor,AI Governance Actor,Almost all dangerous capabilities stem from dual-use skills. Model developers do not have the right incentives for mitigating dangerous capabilities. Governance actors must craft policy here.,responsibility,higher,complete
Actor,AI Governance Actor,"Model deployers are under strong pressures to maximize realized capabilities in dangerous areas (e.g. cybersecurity), and arguably have little obligation to not maximize capabilities. Therefore, AI governance actors do.",responsibility,higher,complete
Actor,AI Governance Actor,"Only developers have reasonable capability to intervene on this, while only governments have a strong obligation to do so on the behalf of their citizens.",responsibility,higher,complete
Actor,Commentary,"I think there's a category error by including AI Governance Actors in this assessment. They're different in kind of users developers and deployers. They're more like a sector, if anything.",general comment,"revisit - check with Zan, what to do with comments at the level of methdological critique?",complete
Actor,AI Governance Actor,FYSA I notice there is significant overlap in my mind between ['Public admin' + 'National security'] and ['AI Governance actors'].,general comment,"revisit - check with Zan, what to do with comments at the level of methdological critique?",complete
Actor,AI Infrastructure Provider,Infrastructure providers act as distribution gatekeepers.,responsibility,higher,complete
Actor,AI Infrastructure Provider,"Deployers and infrastructure providers share high responsibility by controlling access, monitoring misuse, and enforcing policies.",responsibility,higher,complete
Actor,AI Infrastructure Provider,This is a case where infrastructure providers are very vulnerable but have little control due to supply chains that offer them few options for secure components at an affordable price to compete with international markets.,responsibility,lower,complete
Actor,AI Infrastructure Provider,This is a case where infrastructure providers are very vulnerable but have little control due to supply chains that offer them few options for secure components at an affordable price to compete with international markets.,vulnerability,higher,complete
Actor,AI User,"Users and affected stakeholders have limited responsibility, as they are more recipients of risk than active risk managers.",responsibility,lower,complete
Actor,AI User,"While users and affected stakeholders have influence, they should not bear the brunt of mitigation responsibility.",responsibility,lower,complete
Actor,AI User,AI users and stakeholders need to be aware of possible harms.,responsibility,higher,complete
Actor ,AI Governance Actor,Of course malicious users or jailbreakers of models are responsible for their actions (which is why they are listed as moderately responsible) but the main responsibility to mitigate the harm is to prevent it from existing which is on the developers and on governments and other governance professionals to ensure these capabilities are not in the models.,responsibility,higher,complete
Actor,AI Developer (General-purpose AI),Of course malicious users or jailbreakers of models are responsible for their actions (which is why they are listed as moderately responsible) but the main responsibility to mitigate the harm is to prevent it from existing which is on the developers and on governments and other governance professionals to ensure these capabilities are not in the models.,responsibility,higher,complete
Actor,AI User,Of course malicious users or jailbreakers of models are responsible for their actions (which is why they are listed as moderately responsible) but the main responsibility to mitigate the harm is to prevent it from existing which is on the developers and on governments and other governance professionals to ensure these capabilities are not in the models.,responsibility,lower,complete
Actor,AI User,"Individual users, deployers, etc. are not in the causal chain except in their capacity as constituents of the relevant government.",responsibility,lower,complete
Actor,Affected Stakeholder,"Users and affected stakeholders have limited responsibility, as they are more recipients of risk than active risk managers.",responsibility,lower,complete
Actor,Affected Stakeholder,"While users and affected stakeholders have influence, they should not bear the brunt of mitigation responsibility.",responsibility,lower,complete
Actor,Affected Stakeholder,AI users and stakeholders need to be aware of possible harms.,responsibility,higher,complete
Actor,All Actors,"All sectors and actors are at least highly vulnerable because of the large-scale, severe implications of dangerous AI capabilities. The risk is fairly close to being equally distributed across society.",general vulnerability comment,higher,complete
Actor,All Actors,"the risks here are so broad, and so significant, that all actors and ecosystems are highly vulnerable.",general vulnerability comment,higher,complete
Actor,All Actors,All sectors and actors would be vulnerable to catastrophic risks if they were to materialize.,general vulnerability comment,higher,complete
Actor,All Actors,"No actor is not at all responsible, as all actors have some capability to advocate for the alleviation against the development and use of AI systems with dangerous capabilities",general responsibility comment,higher,complete
Actor,All Sectors,"With a broad definition of mass harm, all sectors are vulnerable. A more specific definition may tease out some differences across sectors; however, currently mass harm can be interpreted in many ways",general vulnerability comment,higher,complete
Sector,All Sectors,"The ones affected by AI possessing dangerous capabilities are the ones who are ultimately affected by it. Similar to the previous questions, this isn't sector specific. For example, CBRN and cyber risks can affect anyone, and it is sector-agnostic.",general vulnerability comment,higher,complete
Sector,All Sectors,"There's a base layer of vulnerability that applies across sectors, but not for any reason specific to that sector just because that's the baseline risk which applies across everything.",general vulnerability comment,higher,complete
Sector,All Sectors,"All sectors and actors are at least highly vulnerable because of the large-scale, severe implications of dangerous AI capabilities. The risk is fairly close to being equally distributed across society.",general vulnerability comment,higher,complete
Sector,All Sectors,"the risks here are so broad, and so significant, that all actors and ecosystems are highly vulnerable.",general vulnerability comment,higher,complete
Sector,All Sectors,AIs are general purpose technologies; their risks affect all industries.,general vulnerability comment,higher,complete
Sector,All Sectors,"Overall, the risks of AI possessing dangerous capabilities are quite broad, and should be a concern to all industries.",general vulnerability comment,higher,complete
Sector,All Sectors,All sectors and actors would be vulnerable to catastrophic risks if they were to materialize.,general vulnerability comment,higher,complete
Sector,"Scientific Research, National Security","I rated scientific and national security extreme purely because they will interface with these threats the most, but do note my P(doom) is >50% and technically that does affect everyone.",vulnerability,higher,complete
Sector,Scientific Research,"Scientific use of advanced models offers promising breakthroughs, but with the risk of all the danger that biology, chemistry and physics provide.",vulnerability,higher,complete
Sector,Scientific Research,"Science is the most challenging domain to ensure non-dangerous capabilities are developed, because of the data required for scientific AI.",responsibility,higher,complete
Sector,National Security,Military use of AI adds different exposure because of the actions allowed to AI systems (we have killer robots).,vulnerability,higher,complete
Sector,National Security,"Cyber-escalation may be a dangerous outcome from AI driven military AI enabled cyber-attack systems, especially as attack and defense may not practically be able to be governed by humans in the loop.",responsibility,higher,complete
Sector,"Finance, Transportation, Information Systems, National Security, Scientific R&D","The most vulnerable sectors are those where decision-making scale, automation, and data sensitivity intersect (e.g., finance, transportation, information systems, national security, scientific R&D). These domains are also likely to attract autonomous agent misuse, prompt injection, and adversarial targeting.",vulnerability,higher,complete
Sector,"Real Estate, Entertainment","While sectors like real estate or entertainment may be impacted downstream, their core operations are less vulnerable to AI-generated harm at a systemic level.",vulnerability,lower,complete
Sector,"Information, Finance, Healthcare","Besides impact to science and national security, the sectors of information, finance and health care seem particularly vulnerable because of their increased likelihood of adoption of AI for critical decisions. Harms in these sectors may result in loss of life, disruption of economies, and loss of critical information systems.",vulnerability,higher,complete
Sector,"Finance, Information, National Security","I assume sectors that have significant financial assets, access to sensitive information, and/or relate to national security are more vulnerable.",vulnerability,higher,complete
Sector,Commentary,"It depends on the threat model - AI R&D autonomous situational awareness could be more in GPAI developers or proliferate more rapidly in less savvy commercial AI deployers, manipulative content could be more in users. By sectors, perhaps those with higher saturation of IT systems enabling AI use throughout would result in higher aggregate vulnerability, those wielding power over more vulnerable groups (youth, ill people, high-trust faith-based communities), and those with outsized destructive power (law enforcement and security bodies)",general vulnerability comment,higher,complete
Sector,Commentary,How do you allocate vulnerability to effects like a CBN attack or lss of control of systems?,NA,NA,complete
Sector,National Security,"My main risk driver here is bad actors using dangerous capabilities intentionally to obtain for financial gains, and states using these capabilities to weaken or sabotage other states.",responsibility,higher,complete
Actor,AI User,"My main risk driver here is bad actors using dangerous capabilities intentionally to obtain for financial gains, and states using these capabilities to weaken or sabotage other states.",responsibility,higher,complete
General,Commentary,Not sure how this is particularly industry specific,general comment,general comment,method critique 
General,Commentary,This problem is so deeply entangled with 7.1 that it seems borderline meaningless to give separate vulnerability assessments.,general comment,general comment,method critique 
General,Commentary,"To be clear, this is mostly a future potential harm, but definitely a possible one. I see it as one that is most likely to be driven by human misuse, but also with significant possibility for it to come from AI misbehavior.",general comment,general comment,complete
General,Commentary,"Similar to my reasoning for vulnerability to cyberattacks, weapon development or use, and mass harm. - I assume no actor/sector is not at all vulnerable. Although the use of AI systems with dangerous capabilities are likely to be targeted, there is a non-trivial risk of collateral damage.",general vulnerability comment,general vulnerability comment,complete
General,Commentary,"Unlike other risks that affect actors proportionally, dangerous capabilities create asymmetric vulnerabilities where defenders need perfect protection while attackers need only one success vector. Many dangerous capabilities have legitimate applications, making them harder to restrict.",general vulnerability comment,general vulnerability comment,complete
General,Commentary,"Deceptive scheming in LLMs is well documented, and it is not hard to imagine some promising AI crafted protein that kills many people with an extermination-scheming model (whether through accident of its design, or subverted by attackers to deceive system users).",general comment,general comment,complete
Actor,AI Developer (General-purpose AI),Agentic systems pursuing their own goals must be designed to account for potentially dangerous behavior; assume breach by default (is the zero trust mantra).,responsibility,higher,complete
General,Commentary,This is hard to rate given the likelihood of such an event compared to day risks from discrimination. This is an unideal system for rating such vastly distinct categories of current harms and future risks. An additional axis of timeframe or likelihood needs to accompany these risks which might have an impact on the sectoral rating,general comment,general comment,method critique 
General,Commentary,My ratings are under the assumption that this is about the present state of GPAIs and therefore the likelihood of these risks are low. But it would be significantly different if those parameters were to change,general comment,general comment,method critique 
General,Commentary,"Again, those with the greatest technical capability to understand and control dangerous capabilities bear the highest responsibility.",general responsibility comment,general responsibility comment,complete
Summary,AI Developer (General-purpose AI),"Comments consistently emphasized GPAI developers' primary responsibility as dangerous capabilities stem from how models are trained, requiring extensive investment, with developers having technical control to prevent development while governments have obligations to citizens.",Responsibility,Higher,
Summary,AI Developer (General-purpose AI),"One comment noted vulnerability is indirect, with developers needing to take more precautions against dangerous capabilities.",Vulnerability,Higher,
Summary,AI Developer (Specialized AI),"Comments included specialized developers alongside general-purpose developers as sharing primary responsibility for designing, releasing, and regulating models that could be exploited for large-scale harm.",Responsibility,Higher,
Summary,AI Deployer,"Comments emphasized deployers' high responsibility for securing AI systems, controlling access, monitoring misuse, and enforcing policies, though noting they face strong pressures to maximize capabilities with limited obligations not to.",Responsibility,Higher,
Summary,AI Governance Actor,"Comments overwhelmingly emphasized governance actors' responsibility for crafting policy on dual-use capabilities, enforcing provenance of digital activity, and compelling developers to build safe tools, as model developers lack proper incentives for mitigation.",Responsibility,Higher,
Summary,AI Infrastructure Provider,"Comments characterized infrastructure providers as distribution gatekeepers sharing high responsibility for controlling access and monitoring misuse, though some noted they have limited control due to supply chain constraints.",Responsibility,Higher,
Summary,AI Infrastructure Provider,One comment suggested infrastructure providers have lower responsibility due to limited control from supply chains offering few secure component options at competitive prices.,Responsibility,Lower,
Summary,AI Infrastructure Provider,Comments noted infrastructure providers are very vulnerable due to supply chain limitations affecting their ability to obtain secure components.,Vulnerability,Higher,
Summary,AI User,"Most comments characterized users as having limited responsibility as recipients of risk rather than active risk managers, though malicious users bear responsibility for their actions while main mitigation responsibility lies with developers and governance.",Responsibility,Lower,
Summary,AI User,Some comments noted users need awareness of possible harms and malicious actors may intentionally use dangerous capabilities for financial gains.,Responsibility,Higher,
Summary,Affected Stakeholder,Comments consistently characterized affected stakeholders as having limited responsibility as recipients of risk rather than active risk managers who should not bear the brunt of mitigation responsibility.,Responsibility,Lower,
Summary,Affected Stakeholder,One comment noted stakeholders need to be aware of possible harms.,Responsibility,Higher,
Summary,"Agriculture, Mining, Construction and Manufacturing",No specific vulnerability comments provided for these sectors regarding dangerous capabilities.,Vulnerability,N/A,
Summary,"Trade, Transportation, and Utilities","No specific vulnerability comments provided for these sectors regarding dangerous capabilities, though transportation was mentioned as vulnerable where decision-making scale and automation intersect.",Vulnerability,N/A,
Summary,Information,"Comments identified information as particularly vulnerable due to increased likelihood of AI adoption for critical decisions, with potential for loss of critical information systems and vulnerability to autonomous agent misuse.",Vulnerability,Higher,
Summary,Finance and Insurance,"Comments consistently rated finance as highly to extremely vulnerable due to decision-making scale, automation, and data sensitivity intersecting, with significant financial assets making the sector particularly vulnerable to disruption.",Vulnerability,Higher,
Summary,Real Estate and Rental and Leasing,"Comments suggested real estate faces lower vulnerability as core operations are less vulnerable to AI-generated harm at a systemic level, with impacts being mainly downstream.",Vulnerability,Lower,
Summary,Professional and Technical Services,No specific vulnerability comments provided for this sector regarding dangerous capabilities.,Vulnerability,N/A,
Summary,Scientific Research and Development,"Comments emphasized science as extremely vulnerable, offering promising breakthroughs but with risks from biology, chemistry and physics, being the most challenging domain to ensure non-dangerous capabilities due to required data.",Vulnerability,Higher,
Summary,"Management, Administrative, and Support Services",No specific vulnerability comments provided for this sector regarding dangerous capabilities.,Vulnerability,N/A,
Summary,Educational Services,No specific vulnerability comments provided for this sector regarding dangerous capabilities.,Vulnerability,N/A,
Summary,Health Care and Social Assistance,Comments identified healthcare as particularly vulnerable due to increased likelihood of AI adoption for critical decisions with potential for loss of life from dangerous capabilities.,Vulnerability,Higher,
Summary,"Arts, Entertainment, and Recreation",Comments suggested entertainment faces lower vulnerability as core operations are less vulnerable to AI-generated harm at a systemic level with mainly downstream impacts.,Vulnerability,Lower,
Summary,"Accommodation, Food, and Other Services",No specific vulnerability comments provided for this sector regarding dangerous capabilities.,Vulnerability,N/A,
Summary,Public Administration excluding National Security,No specific vulnerability comments provided for public administration excluding national security regarding dangerous capabilities.,Vulnerability,N/A,
Summary,National Security,"Comments consistently rated national security as extremely vulnerable due to military AI applications, cyber-escalation risks, and states using capabilities to weaken other states, with the sector interfacing with these threats most directly.",Vulnerability,Higher,