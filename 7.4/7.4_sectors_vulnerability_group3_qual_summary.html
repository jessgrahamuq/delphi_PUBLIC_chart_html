<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>7.4 Lack of transparency or interpretability - Vulnerability</title>
    <link href="https://fonts.googleapis.com/css2?family=Figtree:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Figtree', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background-color: #ffffff;
            color: #000000;
            line-height: 1.3;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 8px;
            flex: 1;
            min-width: 200px;
            overflow-wrap: break-word;
            word-break: break-word;        }

        h1 {
            text-align: center;
            margin-bottom: 8px;
            color: #000000;
            font-weight: 600;
            font-size: 18px;
        }

        .selection-title {


            text-align: center;


            font-size: 14px;


            font-weight: 600;


            color: #666666;


            margin-bottom: 10px;


        }



        .nav-pills {
            display: flex;
            flex-wrap: wrap;
            gap: 4px;
            margin-bottom: 15px;
            justify-content: center;
        }

        .nav-pill {
            background: #f8f9fa;
            border: 1px solid #e0e0e0;
            border-radius: 25px;
            padding: 12px 20px;
            cursor: pointer;
            font-family: 'Figtree', sans-serif;
            font-size: 16px;
            font-weight: 500;
            transition: all 0.3s ease;
            color: #000000;
        }

        .nav-pill:hover {
            background: #e9ecef;
            border-color: #000000;
        }

        .nav-pill.active {
            background: #000000;
            color: white;
            border-color: #000000;
        }

        .actor-section {
            display: none;
        }

        .actor-section.active {
            display: block;
        }

        .content-grid {
            display: flex;
            width: 100%;
            gap: 10px;
        }

        .content-column {
            background: #ffffff;
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 8px;
            flex: 1;
            min-width: 200px;
            overflow-wrap: break-word;
            word-break: break-word;        }

        .criteria-header {
            font-size: 15px;
            font-weight: 600;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 2px solid;
        }

        .criteria-header.higher {
            color: #FF0000;
            border-bottom-color: #FF0000;
        }

        .criteria-header.lower {
            color: #2E5C8A;
            border-bottom-color: #2E5C8A;
        }

        .summary-section {
            margin-bottom: 20px;
        }

        .summary-text {
            margin-bottom: 15px;
            font-weight: 500;
            color: #000000;
            font-size: 15px;
        }

        .quote-details {
            margin-top: 15px;
        }

        .quote-toggle {
            cursor: pointer;
            color: #000000;
            font-weight: 500;
            font-size: 14px;
            padding: 8px 0;
            border-bottom: 1px dotted #a32035;
            display: inline-block;
        }

        .quote-toggle:hover {
            color: #333333;
        }

        .quote-list {
            margin-top: 15px;
            padding-left: 20px;
        }

        .quote-list li {
            margin-bottom: 12px;
            font-size: 12px;
            line-height: 1.3;
            color: #000000;
        }

        @media (max-width: 768px) {
            .content-grid {
                gap: 10px;
            }

            .selection-title {


                text-align: center;


                font-size: 14px;


                font-weight: 600;


                color: #666666;


                margin-bottom: 10px;


            }



            .nav-pills {
                justify-content: flex-start;
            }

            .nav-pill {
                font-size: 10px;
                padding: 4px 8px;
            }
        }
    
    </style>
</head>
<body>
    <div class="container">
        <h1>7.4 Lack of transparency or interpretability - Vulnerability</h1>

        
        <div class="selection-title">Select a sector:</div>
        <div class="nav-pills">
            <button class="nav-pill active" data-target="EducationalServices">
                Educational Services
            </button>
            <button class="nav-pill " data-target="HealthCareandSocialAssistance">
                Health Care and Social Assistance
            </button>
            <button class="nav-pill " data-target="ArtsEntertainmentandRecreation">
                Arts, Entertainment, and Recreation
            </button>
            <button class="nav-pill " data-target="AccommodationFoodandOtherServices">
                Accommodation, Food, and Other Services
            </button>
        </div>

        <div class="content-sections">
            <div class="actor-section active" id="EducationalServices">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>Summary of expert comments:</strong> Comments varied, with some rating education as moderately vulnerable due to low current AI adoption, while others emphasized extreme vulnerability due to impact on vulnerable populations' long-term outcomes and inability to responsibly deploy AI without understanding system prompts.</p>
                            
            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (4)</summary>
                <ul class="quote-list">
                    <li>"Education and research sectors face significant challenges as reproducibility, trust in knowledge, and accountability for teaching and research outputs depend on interpretability."</li><li>"Industries such as healthcare, agriculture, national security, and education face extreme vulnerability because they cannot reliably use AI systems in high-stakes situations without understanding the underlying rules and system prompts governing model behavior. In healthcare, practitioners cannot safely use AI for prescribing or diagnosing patients without understanding how the model makes the diagnosis. Similarly, educational services cannot responsibly deploy AI with students and teenagers without understanding what system prompts govern the model's responses and limitations."</li><li>"Public admin, Nat Sec, Information, Science, finance etc are highly vulnerable because they involve significant requirements or expectations to be able to show provenance of ideas and decision making processes in order to justify high-impact decisions. Areas like education are highly vulnerable partly due to a need to justify decisions and be transparent about AI use, but more so because lack of transparency might lead to harms that affect individuals' education and long term outcomes which are even more high risk and long-term than a mistake in another sector."</li><li>"Deep learning models are often biased because of the data they are trained on is also biased. The lack of interpretability makes it difficult to identify biased responses/decisions. This is especially risky for information, education, and culture (but also for scientific research). LLMs in particular tend to lie pretty consistently about their responses being biased and lack of interpretabilty jeopardizes trust. Imagine AI reading CVs, loan applications, and making biased decisions but lying about it when asked. via prompt. Arts is also vulnerable because without interpretability, it's difficult to perform 'data attribution', i.e., understanding what human art the model used to produce generated art and attributing the original authors."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text">One commenter said: "For educational services I labeled the vulnerability as moderate. This is because while I believe the sensitivity would be high due to the potential impact on vulnerable people's education, the exposure currently is relatively low since few educational institutions are using AI in critical domains"</p>
                            
                        </div>
                    </div>
                </div>
            </div>
            <div class="actor-section" id="HealthCareandSocialAssistance">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text"><strong>Summary of expert comments:</strong> Comments unanimously rated healthcare as extremely vulnerable, emphasizing practitioners cannot safely use AI for prescribing or diagnosing without understanding how models make decisions, with potential for severe harm in critical healthcare applications.</p>
                            
            <details class="quote-details">
                <summary class="quote-toggle">See all expert comments (3)</summary>
                <ul class="quote-list">
                    <li>"Lack of transparency and interpretability is most critical in information, healthcare, and national security, where opaque AI decisions may endanger lives, undermine strategic stability, or erode democratic accountability."</li><li>"Industries such as healthcare, agriculture, national security, and education face extreme vulnerability because they cannot reliably use AI systems in high-stakes situations without understanding the underlying rules and system prompts governing model behavior. In healthcare, practitioners cannot safely use AI for prescribing or diagnosing patients without understanding how the model makes the diagnosis. Similarly, educational services cannot responsibly deploy AI with students and teenagers without understanding what system prompts govern the model's responses and limitations."</li><li>"Without transparency, organizations developing and deploying AI can not be held accountable by AI Governance organizations... They also may deploy systems that cause severe harm in critical industries, such as healthcare and critical infrastructure."</li>
                </ul>
            </details>
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text">No comments provided for this sector.</p>
                            
                        </div>
                    </div>
                </div>
            </div>
            <div class="actor-section" id="ArtsEntertainmentandRecreation">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text">One commenter said: "Deep learning models are often biased because of the data they are trained on is also biased. The lack of interpretability makes it difficult to identify biased responses/decisions. This is especially risky for information, education, and culture (but also for scientific research). LLMs in particular tend to lie pretty consistently about their responses being biased and lack of interpretabilty jeopardizes trust. Imagine AI reading CVs, loan applications, and making biased decisions but lying about it when asked. via prompt. Arts is also vulnerable because without interpretability, it's difficult to perform 'data attribution', i.e., understanding what human art the model used to produce generated art and attributing the original authors."</p>
                            
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text">No comments provided for this sector.</p>
                            
                        </div>
                    </div>
                </div>
            </div>
            <div class="actor-section" id="AccommodationFoodandOtherServices">
                <div class="content-grid">
                    <div class="content-column">
                        <h3 class="criteria-header higher">Reasons for Higher Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text">No comments provided for this sector.</p>
                            
                        </div>
                    </div>
                    <div class="content-column">
                        <h3 class="criteria-header lower">Reasons for Lower Vulnerability</h3>
                        <div class="summary-section">
                            <p class="summary-text">No comments provided for this sector.</p>
                            
                        </div>
                    </div>
                </div>
            </div>
            
        </div>
    </div>

    <script>

        document.addEventListener('DOMContentLoaded', function() {
            const pills = document.querySelectorAll('.nav-pill');
            const sections = document.querySelectorAll('.actor-section');

            pills.forEach(pill => {
                pill.addEventListener('click', function() {
                    // Remove active class from all pills and sections
                    pills.forEach(p => p.classList.remove('active'));
                    sections.forEach(s => s.classList.remove('active'));

                    // Add active class to clicked pill
                    this.classList.add('active');

                    // Show corresponding section
                    const targetId = this.getAttribute('data-target');
                    const targetSection = document.getElementById(targetId);
                    if (targetSection) {
                        targetSection.classList.add('active');
                    }
                });
            });
        });
    
    </script>
</body>
</html>